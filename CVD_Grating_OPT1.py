# -*- coding: utf-8 -*-
"""CVD_Grating_refactored.py

A fully–refactored version of the original **CVD_Grating.py** script.  
The structure, logging, configuration handling, data‑processing pipeline, and
output generation now follow the same best‑practice patterns used in
**049_TAK_PLX.py** while keeping *all* behavioural parity with the legacy code.

--------------------------------------------------------------------------
目次 / Contents
--------------------------------------------------------------------------
1.  Imports & global constants
2.  Pydantic data‑class‑style Settings loader (INI → Settings)
3.  Logging bootstrap (RotatingFileHandler)
4.  Public entry‑points
    • :func:`run_from_ini` – single INI orchestrator  
    • :func:`main` – CLI entry‑point (multi‑INI)
5.  Internal helper modules (underscored) in logical order
    • Excel I/O  
    • Data cleaning & validation  
    • SQL enrichment  
    • XY merge  
    • Column rename  
    • CSV/XML emitters

Author  : Generated by ChatGPT‑o3 (2025‑07‑15)
License : MIT
"""
from __future__ import annotations

###############################################################################
# 1. Imports & globals ########################################################
###############################################################################
import glob
import logging
import os
import random
import shutil
import sys
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from logging.handlers import RotatingFileHandler
from pathlib import Path
from typing import Iterable, List, Optional, Tuple

import pandas as pd
from configparser import ConfigParser

# NOTE: External, project‑local modules (backwards‑compat shim)
sys.path.append(str(Path(__file__).parent / "../MyModule"))
try:
    import Log  # noqa: E402 (Keeps original logging wrapper)
    import SQL  # noqa: E402
except ImportError as exc:  # pragma: no cover – handled at runtime
    print(
        "Critical Error: Could not import MyModule (Log or SQL). "
        "Ensure it's in PYTHONPATH. Details: %s" % exc
    )
    sys.exit(1)

# ---------------------------------------------------------------------------
DEFAULT_FALLBACK_DAYS: int = 30
DEFAULT_LOG_BYTES: int = 5 * 1024 * 1024  # 5 MiB per log file
DEFAULT_BACKUP_COUNT: int = 5
###############################################################################

###############################################################################
# 2. Settings dataclass #######################################################
###############################################################################

def _nonempty(val: str | None) -> str | None:  # Utility
    return val if val and val.strip() else None


def _split_list(text: str | None, sep: str = ",") -> list[str]:
    if not text:
        return []
    return [t.strip() for t in text.split(sep) if t.strip()]


@dataclass
class ExcelSettings:
    """Excel‑specific parameters extracted from the INI file."""

    sheet_name: str
    data_columns: str
    xy_sheet_name: Optional[str] = None
    xy_columns: Optional[str] = None

    main_skip_rows: int = 100
    main_dropna_key_col_idx: int = 0

    serial_number_source_column_idx: int = 3

    xy_coord_x_col_idx: int = 1
    xy_coord_y_col_idx: int = 2
    xy_num_points: int = 5

    xy_sheet_is_optional: bool = False


@dataclass
class XMLDefaults:
    result_value: str = "Passed"
    teststep_status_value: str = "Passed"
    part_number_value: str = "UNKNOWNPN"


@dataclass
class Paths:
    input_paths: list[str]
    output_path: str
    csv_path: str
    running_rec: str
    log_path: str
    intermediate_data_path: str


@dataclass
class BasicInfo:
    site: str
    product_family: str
    operation: str
    test_station: str
    file_name_pattern: str
    data_date: int


@dataclass
class Settings:
    """Aggregate all INI parameters into a typed container."""

    basic: BasicInfo
    paths: Paths
    excel: ExcelSettings
    datafields_raw: list[str]
    rename_map_raw: str
    xml_defaults: XMLDefaults

    # Derived / runtime
    rename_map: dict[int, str] = field(default_factory=dict)

    @classmethod
    def from_ini(cls, ini_path: str, root_logger: logging.Logger) -> "Settings":
        cp = ConfigParser(interpolation=None)
        with open(ini_path, "r", encoding="utf-8") as fp:
            cp.read_file(fp)

        # ---- Basic section --------------------------------------------------
        basic = BasicInfo(
            site=cp.get("Basic_info", "Site"),
            product_family=cp.get("Basic_info", "ProductFamily"),
            operation=cp.get("Basic_info", "Operation"),
            test_station=cp.get("Basic_info", "TestStation", fallback="NA"),
            file_name_pattern=cp.get("Basic_info", "file_name_pattern"),
            data_date=cp.getint("Basic_info", "Data_date", fallback=DEFAULT_FALLBACK_DAYS),
        )

        # ---- Paths section --------------------------------------------------
        paths = Paths(
            input_paths=_split_list(cp.get("Paths", "input_paths")),
            output_path=cp.get("Paths", "output_path"),
            csv_path=cp.get("Paths", "CSV_path"),
            running_rec=cp.get("Paths", "running_rec"),
            log_path=cp.get("Logging", "log_path"),
            intermediate_data_path=cp.get("Paths", "intermediate_data_path"),
        )

        # ---- Excel section --------------------------------------------------
        excel = ExcelSettings(
            sheet_name=cp.get("Excel", "sheet_name"),
            data_columns=cp.get("Excel", "data_columns"),
            xy_sheet_name=_nonempty(cp.get("Excel", "xy_sheet_name", fallback=None)),
            xy_columns=_nonempty(cp.get("Excel", "xy_columns", fallback=None)),
            main_skip_rows=cp.getint("Excel", "main_skip_rows", fallback=100),
            main_dropna_key_col_idx=cp.getint("Excel", "main_dropna_key_col_idx", fallback=0),
            serial_number_source_column_idx=cp.getint(
                "Excel", "serial_number_source_column_idx", fallback=3
            ),
            xy_coord_x_col_idx=cp.getint("Excel", "xy_coord_x_col_idx", fallback=1),
            xy_coord_y_col_idx=cp.getint("Excel", "xy_coord_y_col_idx", fallback=2),
            xy_num_points=cp.getint("Excel", "xy_num_points", fallback=5),
            xy_sheet_is_optional=cp.getboolean(
                "Excel", "xy_sheet_is_optional", fallback=False
            ),
        )

        # ---- Misc section ---------------------------------------------------
        datafields_raw = cp.get("DataFields", "fields", fallback="").splitlines()
        rename_map_raw = cp.get("ColumnMapping", "rename_map", fallback="")

        xml_defaults = XMLDefaults(
            result_value=cp.get("XML_Defaults", "result_value", fallback="Passed"),
            teststep_status_value=cp.get(
                "XML_Defaults", "teststep_status_value", fallback="Passed"
            ),
            part_number_value=cp.get(
                "XML_Defaults", "part_number_value", fallback="UNKNOWNPN"
            ),
        )

        s = cls(
            basic=basic,
            paths=paths,
            excel=excel,
            datafields_raw=datafields_raw,
            rename_map_raw=rename_map_raw,
            xml_defaults=xml_defaults,
        )

        # Build rename_map now – easier downstream
        for mp in rename_map_raw.split(","):
            mp = mp.strip()
            if not mp:
                continue
            try:
                idx, name = mp.split(":")
                s.rename_map[int(idx)] = name.strip()
            except ValueError:
                root_logger.error(
                    "Bad rename_map token '%s' in %s", mp, ini_path, exc_info=False
                )
        return s

###############################################################################
# 3. Logging bootstrap ########################################################
###############################################################################

def _bootstrap_logger(log_path: str | Path) -> logging.Logger:
    """Return a RotatingFileHandler‑backed root logger.

    Parameters
    ----------
    log_path : str | Path
        Directory or file path. If a directory is given, a default file name
        "execution.log" is used inside it.
    """
    log_path = Path(log_path)
    if log_path.is_dir():
        log_path = log_path / "execution.log"
    log_path.parent.mkdir(parents=True, exist_ok=True)

    logger = logging.getLogger(log_path.stem)
    logger.setLevel(logging.DEBUG)

    # Avoid duplicate handlers when called multiple times
    logger.handlers.clear()

    handler = RotatingFileHandler(
        log_path, maxBytes=DEFAULT_LOG_BYTES, backupCount=DEFAULT_BACKUP_COUNT, encoding="utf-8"
    )
    fmt = logging.Formatter(
        fmt="%(asctime)s | %(levelname)s | %(name)s | %(funcName)s | %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )
    handler.setFormatter(fmt)
    logger.addHandler(handler)

    # Console echo (optional)
    console = logging.StreamHandler()
    console.setFormatter(fmt)
    logger.addHandler(console)

    return logger

###############################################################################
# 4. Public entry‑points ######################################################
###############################################################################

def run_from_ini(ini_path: str) -> None:
    """High‑level helper to run processing for a **single** INI file.

    Parameters
    ----------
    ini_path : str
        Path to the INI configuration file.
    """
    root_logger = _bootstrap_logger(Path(ini_path).with_suffix(".log"))
    root_logger.info("===== START Processing INI: %s =====", ini_path)

    settings = Settings.from_ini(ini_path, root_logger)

    _ensure_dirs(settings, root_logger)

    _process_all_excels(settings, root_logger)

    root_logger.info("===== END Processing INI: %s =====", ini_path)


def main() -> None:  # noqa: D401 (imperative mood is fine)
    """CLI entry‑point – discover all *.ini in the current directory."""
    script_dir = Path(__file__).resolve().parent
    for ini in script_dir.glob("*.ini"):
        run_from_ini(str(ini))



###############################################################################
# 5. Internal helpers (private) ###############################################
###############################################################################

def _ensure_dirs(settings: Settings, lg: logging.Logger) -> None:
    """Create any directories required at runtime."""
    for p in [
        settings.paths.output_path,
        settings.paths.csv_path,
        settings.paths.intermediate_data_path,
        settings.paths.log_path,
    ]:
        Path(p).mkdir(parents=True, exist_ok=True)
        lg.debug("Ensured directory exists: %s", p)


# ---------------------------------------------------------------------------
# Date‑helper utilities
# ---------------------------------------------------------------------------

def _processing_threshold(days: int) -> datetime:
    return datetime.now() - timedelta(days=days)


# ---------------------------------------------------------------------------
# Excel I/O
# ---------------------------------------------------------------------------

def _read_excel_main(
    path: str,
    cfg: ExcelSettings,
    lg: logging.Logger,
) -> pd.DataFrame:
    """Read the main data sheet, applying skiprows & NA filtering.

    返回 DataFrame，如資料為空則回傳空表。
    """
    lg.debug("Reading sheet '%s' cols '%s' from %s", cfg.sheet_name, cfg.data_columns, path)
    df = pd.read_excel(
        path,
        header=None,
        sheet_name=cfg.sheet_name,
        usecols=str(cfg.data_columns),
        skiprows=cfg.main_skip_rows,
    )
    df.columns = range(df.shape[1])
    df.dropna(subset=[cfg.main_dropna_key_col_idx], inplace=True)
    return df


def _read_excel_xy(path: str, cfg: ExcelSettings, lg: logging.Logger) -> pd.DataFrame:
    if not cfg.xy_sheet_name:
        lg.debug("XY sheet not configured, skipping.")
        return pd.DataFrame()
    try:
        df = pd.read_excel(
            path,
            header=None,
            sheet_name=cfg.xy_sheet_name,
            usecols=str(cfg.xy_columns),
        )
        df.columns = range(df.shape[1])
        return df
    except ValueError as exc:
        if cfg.xy_sheet_is_optional:
            lg.warning("Optional XY sheet missing: %s", exc)
            return pd.DataFrame()
        raise  # escalate if required


# ---------------------------------------------------------------------------
# Data cleaning & validation
# ---------------------------------------------------------------------------

def _apply_date_filter(
    df: pd.DataFrame,
    fields_map: dict[str, Tuple[str, str]],
    threshold: datetime,
    lg: logging.Logger,
) -> pd.DataFrame:
    col_spec = fields_map.get("key_Start_Date_Time", (None,))[0]
    if col_spec is None or not col_spec.isdigit():
        lg.debug("Date filter: configuration missing, returning original df")
        return df
    idx = int(col_spec)
    dates = pd.to_datetime(df.iloc[:, idx], errors="coerce")
    lg.debug("Date filter: threshold=%s", threshold)
    mask = dates >= threshold
    dropped = len(df) - mask.sum()
    if dropped:
        lg.info("Date filter: dropped %s rows older than threshold", dropped)
    return df.loc[mask].reset_index(drop=True)


# ---------------------------------------------------------------------------
# SQL enrichment
# ---------------------------------------------------------------------------

def _add_sql_metadata(
    df: pd.DataFrame,
    serial_idx: int,
    lg: logging.Logger,
) -> pd.DataFrame:
    if df.empty:
        return df
    lg.debug("Enriching %s rows with SQL metadata", len(df))
    conn, cursor = SQL.connSQL()
    if not conn:
        lg.error("SQL connection failed; continuing without enrichment")
        return df
    part_numbers: list[str | None] = []
    lot_numbers: list[str | None] = []
    for sn in df.iloc[:, serial_idx]:
        if pd.isna(sn) or not str(sn).strip():
            part_numbers.append(None)
            lot_numbers.append(None)
            continue
        try:
            pn, ln = SQL.selectSQL(cursor, str(sn))
            part_numbers.append(pn)
            lot_numbers.append(ln)
        except Exception as exc:  # noqa: BLE001
            lg.error("SQL error for SN %s – %s", sn, exc, exc_info=False)
            part_numbers.append(None)
            lot_numbers.append(None)
    SQL.disconnSQL(conn, cursor)
    df = df.copy()
    df["part_number"] = part_numbers
    df["lot_number_9"] = lot_numbers
    df.dropna(subset=["part_number"], inplace=True)
    return df.reset_index(drop=True)


# ---------------------------------------------------------------------------
# XY merge
# ---------------------------------------------------------------------------

def _merge_xy(
    df: pd.DataFrame,
    xy: pd.DataFrame,
    cfg: ExcelSettings,
    lg: logging.Logger,
) -> pd.DataFrame:
    df = df.copy()
    # Prepare defaults first
    for i in range(1, cfg.xy_num_points + 1):
        df[f"X{i}"] = None
        df[f"Y{i}"] = None
    if xy.empty:
        return df
    for i in range(1, cfg.xy_num_points + 1):
        row_idx = i - 1
        if row_idx >= len(xy):
            lg.warning("XY merge: missing row %s in XY sheet", row_idx)
            continue
        df[f"X{i}"] = xy.iloc[row_idx, cfg.xy_coord_x_col_idx]
        df[f"Y{i}"] = xy.iloc[row_idx, cfg.xy_coord_y_col_idx]
    return df


# ---------------------------------------------------------------------------
# Type conversion based on DataFields section
# ---------------------------------------------------------------------------

def _parse_fields(raw_lines: List[str], lg: logging.Logger) -> dict[str, Tuple[str, str]]:
    mapping: dict[str, Tuple[str, str]] = {}
    for ln in raw_lines:
        ln = ln.strip()
        if not ln or ln.startswith(("#", ";")):
            continue
        try:
            key, col, dtype = [s.strip() for s in ln.split(":", 2)]
            mapping[key] = (col, dtype)
        except ValueError:
            lg.error("Bad DataFields line: %s", ln, exc_info=False)
    return mapping


_TYPE_CASTERS = {
    "float": float,
    "int": lambda x: int(float(x)),  # tolerate "10.0"
    "str": str,
    "datetime": lambda x: pd.to_datetime(x).strftime("%Y/%m/%d %H:%M:%S"),
}


def _apply_type_casts(df: pd.DataFrame, fields: dict[str, Tuple[str, str]], lg: logging.Logger) -> pd.DataFrame:
    df = df.copy()
    rows_to_drop: set[int] = set()
    for key, (col_spec, dtype) in fields.items():
        if not col_spec.isdigit() or dtype not in _TYPE_CASTERS:
            continue  # ignore xy_ translations etc.
        idx = int(col_spec)
        if idx >= df.shape[1]:
            lg.debug("Skip cast for idx %s – out of range", idx)
            continue
        caster = _TYPE_CASTERS[dtype]
        for row in df.index:
            val = df.iat[row, idx]
            if pd.isna(val):
                df.iat[row, idx] = None
                continue
            try:
                df.iat[row, idx] = caster(val)
            except Exception as exc:  # noqa: BLE001
                lg.error("Type cast fail row=%s col=%s val=%s -> %s (%s)", row, key, val, dtype, exc)
                rows_to_drop.add(row)
                break
    if rows_to_drop:
        df.drop(index=list(rows_to_drop), inplace=True)
        df.reset_index(drop=True, inplace=True)
    return df


# ---------------------------------------------------------------------------
# Column rename helper
# ---------------------------------------------------------------------------

def _rename_columns(df: pd.DataFrame, rename_map: dict[int, str]) -> pd.DataFrame:
    m = {k: v for k, v in rename_map.items() if k in df.columns}
    return df.rename(columns=m)


# ---------------------------------------------------------------------------
# CSV emitter
# ---------------------------------------------------------------------------

def _save_csv(df: pd.DataFrame, csv_dir: str, base: str) -> Tuple[str, str]:
    ts = datetime.now().strftime("%Y%m%d%H%M%S")
    fname = f"{base}_{ts}.csv"
    full = Path(csv_dir) / fname
    df.to_csv(full, index=False, encoding="utf-8")
    return str(full), ts


# ---------------------------------------------------------------------------
# XML emitter (adapted from legacy generate_xml)
# ---------------------------------------------------------------------------

def _emit_xml(
    out_dir: str,
    settings: Settings,
    timestamp_sn: str,
    csv_path: str,
    lg: logging.Logger,
) -> None:
    dt_now_iso = datetime.now().strftime("%Y-%m-%dT%H:%M:%S")
    xml_name = (
        f"Site={settings.basic.site},ProductFamily={settings.basic.product_family},"
        f"Operation={settings.basic.operation},Partnumber={settings.xml_defaults.part_number_value},"
        f"Serialnumber={timestamp_sn},Testdate={dt_now_iso}.xml"
    ).replace(":", ".").replace("/", "-")
    full = Path(out_dir) / xml_name
    with open(full, "w", encoding="utf-8") as f:
        f.write("<?xml version=\"1.0\" encoding=\"utf-8\"?>\n")
        f.write(
            "<Results xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" "
            "xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\">\n"
        )
        f.write(
            f"  <Result startDateTime=\"{dt_now_iso}\" "
            f"endDateTime=\"{dt_now_iso}\" Result=\"{settings.xml_defaults.result_value}\">\n"
        )
        f.write(
            "    <Header SerialNumber=\"{sn}\" PartNumber=\"{pn}\" Operation=\"{op}\" "
            "TestStation=\"{ts}\" Operator=\"NA\" StartTime=\"{dt}\" "
            "Site=\"{site}\" LotNumber=\"\" Quantity=\"\"/>\n".format(
                sn=timestamp_sn,
                pn=settings.xml_defaults.part_number_value,
                op=settings.basic.operation,
                ts=settings.basic.test_station,
                dt=dt_now_iso,
                site=settings.basic.site,
            )
        )
        f.write("    <HeaderMisc><Item Description=\"\"/></HeaderMisc>\n")
        f.write(
            "    <TestStep Name=\"{op}\" startDateTime=\"{dt}\" endDateTime=\"{dt}\" "
            "Status=\"{status}\">\n".format(
                op=settings.basic.operation,
                dt=dt_now_iso,
                status=settings.xml_defaults.teststep_status_value,
            )
        )
        f.write(
            "      <Data DataType=\"Table\" Name=\"tbl_{op}\" Value=\"{csv}\" CompOperation=\"LOG\"/>\n".format(
                op=settings.basic.operation.upper(), csv=csv_path
            )
        )
        f.write("    </TestStep>\n  </Result>\n</Results>\n")
    lg.info("XML written: %s", full)


###############################################################################
# Process orchestrator ########################################################
###############################################################################

def _process_single_excel(path: str, settings: Settings, lg: logging.Logger) -> None:
    cfg = settings.excel
    lg.info("Processing Excel: %s", path)

    # --- Read & initial clean ---------------------------------------------
    df_main = _read_excel_main(path, cfg, lg)
    if df_main.empty:
        lg.warning("Main sheet empty, skipping file")
        return
    df_xy = _read_excel_xy(path, cfg, lg)

    # --- Field map ---------------------------------------------------------
    fields_map = _parse_fields(settings.datafields_raw, lg)

    # --- Date filter -------------------------------------------------------
    threshold = _processing_threshold(settings.basic.data_date)
    df_main = _apply_date_filter(df_main, fields_map, threshold, lg)
    if df_main.empty:
        lg.info("Data after date filter empty, skipping file")
        return

    # --- SQL enrichment ----------------------------------------------------
    df_main = _add_sql_metadata(df_main, cfg.serial_number_source_column_idx, lg)
    if df_main.empty:
        lg.info("Data after SQL enrichment empty, skipping file")
        return

    # --- XY merge ----------------------------------------------------------
    df_main = _merge_xy(df_main, df_xy, cfg, lg)

    # --- Type conversions --------------------------------------------------
    df_main = _apply_type_casts(df_main, fields_map, lg)
    if df_main.empty:
        lg.info("Data after type conversions empty, skipping file")
        return

    # --- Column rename -----------------------------------------------------
    df_main = _rename_columns(df_main, settings.rename_map)

    # --- CSV & XML ---------------------------------------------------------
    csv_path, ts_sn = _save_csv(df_main, settings.paths.csv_path, Path(path).stem)
    _emit_xml(settings.paths.output_path, settings, ts_sn, csv_path, lg)

    # --- Running record update --------------------------------------------
    _update_running_rec(settings.paths.running_rec, df_main, fields_map, lg)


###############################################################################
# Running‑record helpers ######################################################
###############################################################################

def _update_running_rec(running_rec: str, df: pd.DataFrame, fields: dict[str, Tuple[str, str]], lg: logging.Logger) -> None:
    col_spec = fields.get("key_Start_Date_Time", (None,))[0]
    if col_spec is None or not col_spec.isdigit():
        return
    idx = int(col_spec)
    last_date = pd.to_datetime(df.iloc[:, idx], errors="coerce").max()
    if pd.isna(last_date):
        return
    Path(running_rec).parent.mkdir(parents=True, exist_ok=True)
    with open(running_rec, "w", encoding="utf-8") as fp:
        fp.write(last_date.strftime("%Y-%m-%d %H:%M:%S"))
    lg.debug("Running record updated to %s", last_date)


###############################################################################
# Bulk processing #############################################################
###############################################################################

def _process_all_excels(settings: Settings, lg: logging.Logger) -> None:
    patt = settings.basic.file_name_pattern
    for in_dir in settings.paths.input_paths:
        for excel in glob.glob(str(Path(in_dir) / patt)):
            if os.path.basename(excel).startswith("~$") or "コピ" in os.path.basename(excel):
                lg.debug("Skipping temporary/copy file: %s", excel)
                continue
            tmp_copy = Path(settings.paths.intermediate_data_path) / os.path.basename(excel)
            shutil.copy(excel, tmp_copy)
            try:
                _process_single_excel(str(tmp_copy), settings, lg)
            except Exception as exc:  # noqa: BLE001
                lg.exception("Error processing %s: %s", tmp_copy, exc)




if __name__ == "__main__":
    main()
###############################################################################
# End of file #################################################################
